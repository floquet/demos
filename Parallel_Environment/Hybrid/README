
Example of hybrid MPI and OpenMP.
---------------------------------

Copy these files to a subdirectory of your $HOME or $WORKDIR.

The source code is xthi.c .  It lists the node name and core assignment
of each OpenMP thread.

This example shows how to compile the program using the Intel compiler.

The utilization of two different MPI environments are shown,
MPT (SGI MPI) and IMPI (Intel MPI).
 [As of June 9, 2015, it is not possible to launch IMPI jobs.
  A job batch script that works on other SGI Altix ICE machines
  does not work on topaz.
  Changes in documentation of IMPI will be made when the problem
  has been solved.]

The default MPI of topaz is MPT.  The "module list" command should
show some version of sgimpt, for example

> module list
Currently Loaded Modulefiles:
  1) mpi/sgimpt/2.12

To use Intel MPI the module swap command would be

module swap mpi/sgimpt mpi/intelmpi

The module names could also have a version number.

Before submitting a job, the user needs to compile and link
the program using the following command.  (The option "-O3" is
optional. Typically users will want to have compiler optimization.)

Either

icc -O3 -openmp -o mpt_xthi.exe xthi.c -lmpi  # SGI MPT

or

mpiicc -O3 -openmp -o impi_xthi.exe xthi.c   # Intel MPI

depending on whether you are using  mpi/sgimpt or mpi/intelmpi .

In the MPT environment, a program can be compiled with mpicc,
but for mpicc the default compiler is gcc.  [As of June 9, 2015,
this may change in the future.]  The compiler needs to be set
using the environment variable MPICC_CC, as shown below.

sh, ksh, bash

export MPICC_CC=icc
mpicc -O3 -openmp -o mpt_xthi.exe xthi.c  # SGI MPT

csh, tcsh

setenv MPICC_CC icc
mpicc -O3 -openmp -o mpt_xthi.exe xthi.c  # SGI MPT

An executable compiled for SGI MPT can be submitted with the script
mpt_xthi.pbs.  Notice that the executable is launched with
mpiexec_mpt.  For SGI MPT, do not use mpirun to launch the job.

An executable compiled for Intel MPI can be submitted with the script
impi_xthi.pbs.  Notice that the executable is launched with mpirun.

In the batch script, change the account number to the value assigned
to your project.

All memory on a node is shared by all the cores, but the access time
is not uniform.  This is called Non-Uniform Memory Access (NUMA).
A group of cores that share memory with the fastest access time
for that group is called a "NUMA" or a "NUMA node".  (To clarify
the meaning, it would be more logical to call the group an UMA.)
Each compute node on topaz with 36 cores has 4 9-way NUMAs.  This
example puts 4 MPI processes on each node and uses 9 OpenMP threads
per MPI process.  Each thread group utilizes one NUMA.  In the MPT
environment, the tool dplace can be used to control the placement
of OpenMP threads.  Here is a description of the batch job script
mpt_xthi.pbs .

#PBS -l select=2:ncpus=36:mpiprocs=4

[...]

export OMP_NUM_THREADS=9
# setenv OMP_NUM_THREADS 9

mpiexec_mpt -np 8 omplace -s 1 -c 0-35:bs=${OMP_NUM_THREADS}+st=9 ./mpt_xthi.exe

Each node has 4 MPI processes and each MPI process has 9 threads.
The omplace option "-c 0-35:bs=${OMP_NUM_THREADS}+st=9" specifies to
start the first OpenMP thread on core 0.  Moreover, blocks of
"bs=${OMP_NUM_THREADS}" threads are assigned to cores and different
blocks are assigned with a step size "st=9".  The omplace option
"-s 1" is due to an attribute of MPT MPI jobs, specifically, the first
thread on each node is a shepherd process which uses very little CPU time.

Having more than 9 OpenMP threads per MPI process may be less efficient
than having 9 or less because more than 9 threads will span more than
one NUMA node.

